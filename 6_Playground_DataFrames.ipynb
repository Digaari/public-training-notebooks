{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6.Playground_DataFrames.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXatvRX899i0"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt-CiWRewNWD"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop//blob/master/tutorials/Certification_Trainings/Public/6.Playground_DataFrames.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Alh8i-_fJ59"
      },
      "source": [
        "# Spark DataFrames Playground"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ0MnF3bfWbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abcb416-a817-402e-c653-fca262156910"
      },
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
        "# !bash colab.sh\n",
        "# by default they are set to the latest\n",
        "# -p is for pyspark\n",
        "# -s is for spark-nlp\n",
        "# setup colab with a specific version:\n",
        "# !bash colab.sh -p 3.1.1 -s 3.0.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-12 16:28:51--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.26\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.26|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2021-04-12 16:28:51--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1593 (1.6K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.56K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-12 16:28:52 (42.5 MB/s) - written to stdout [1593/1593]\n",
            "\n",
            "setup Colab for PySpark 3.1.1 and Spark NLP 3.0.1\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 81kB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 56.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 24.5MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZucC6lQ2pYQ"
      },
      "source": [
        "\n",
        "<b>  if you want to work with Spark 2.3 </b>\n",
        "```\n",
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get update -qq\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz\n",
        "\n",
        "!tar xf spark-2.3.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.0-bin-hadoop2.7\"\n",
        "! java -version\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "! pip install --ignore-installed -q spark-nlp==2.7.5\n",
        "\n",
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start(spark23=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4wDHHvKfHyO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "a0ef3d02-9226-417a-f8e3-67682847a45e"
      },
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "\n",
        "print(\"Apache Spark version:\", spark.version)\n",
        "\n",
        "spark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP version 3.0.1\n",
            "Apache Spark version: 3.1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://aa51764978e0:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fa2dd951e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi7g0dR4fDhD"
      },
      "source": [
        "spark = sparknlp.start()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLulYEvRfDhG"
      },
      "source": [
        "document = DocumentAssembler().setInputCol('text').setOutputCol('document')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLfu1NU_fDhJ"
      },
      "source": [
        "tokenizer = Tokenizer().setInputCols('document').setOutputCol('token')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxZ9s3YCfDhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1046bd6e-19fc-4434-84d8-270ea5bdbea4"
      },
      "source": [
        "pos = PerceptronModel.pretrained().setInputCols('document', 'token').setOutputCol('pos')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 3.9 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI-uo9ZbfDhR"
      },
      "source": [
        "pipeline = Pipeline().setStages([document, tokenizer, pos])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJoifWwRfjrG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472b6789-23d8-4013-8e6d-2fb8d921810e"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jupyter/annotation/english/spark-nlp-basics/sample-sentences-en.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-12 16:31:05--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jupyter/annotation/english/spark-nlp-basics/sample-sentences-en.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 284 [text/plain]\n",
            "Saving to: ‘sample-sentences-en.txt’\n",
            "\n",
            "sample-sentences-en 100%[===================>]     284  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-12 16:31:05 (19.2 MB/s) - ‘sample-sentences-en.txt’ saved [284/284]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGksf1hCfDhU"
      },
      "source": [
        "data = spark.read.text('./sample-sentences-en.txt').toDF('text')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgTNu0_KfDhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33818bef-e0ab-4f3c-8188-c3810e7d5a1b"
      },
      "source": [
        "data.show(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|Peter is a very g...|\n",
            "|My life in Russia...|\n",
            "|John and Peter ar...|\n",
            "|Lucas Nogal Dunbe...|\n",
            "|Europe is very cu...|\n",
            "+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAnijYcYfDhb"
      },
      "source": [
        "model = pipeline.fit(data)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_fTTzUPfDhd"
      },
      "source": [
        "result = model.transform(data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll7rPvWefDhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c0861ff-8d73-4eb1-cdae-b2174ce0ba2d"
      },
      "source": [
        "result.show(5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|               token|                 pos|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[{document, 0, 27...|[{token, 0, 4, Pe...|[{pos, 0, 4, NNP,...|\n",
            "|My life in Russia...|[{document, 0, 37...|[{token, 0, 1, My...|[{pos, 0, 1, PRP$...|\n",
            "|John and Peter ar...|[{document, 0, 76...|[{token, 0, 3, Jo...|[{pos, 0, 3, NNP,...|\n",
            "|Lucas Nogal Dunbe...|[{document, 0, 67...|[{token, 0, 4, Lu...|[{pos, 0, 4, NNP,...|\n",
            "|Europe is very cu...|[{document, 0, 68...|[{token, 0, 5, Eu...|[{pos, 0, 5, NNP,...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i06WW8wzfDhk"
      },
      "source": [
        "stored = result\\\n",
        "  .select('text', 'pos.begin', 'pos.end', 'pos.result', 'pos.metadata')\\\n",
        "  .toDF('text', 'pos_begin', 'pos_end', 'pos_result', 'pos_meta')\\\n",
        "  .cache()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBQWLPjzfDhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf87a0c-e2cc-48d8-dae6-ef4171f36e17"
      },
      "source": [
        "stored.printSchema()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- pos_begin: array (nullable = true)\n",
            " |    |-- element: integer (containsNull = true)\n",
            " |-- pos_end: array (nullable = true)\n",
            " |    |-- element: integer (containsNull = true)\n",
            " |-- pos_result: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- pos_meta: array (nullable = true)\n",
            " |    |-- element: map (containsNull = true)\n",
            " |    |    |-- key: string\n",
            " |    |    |-- value: string (valueContainsNull = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X93ASKmGfDhw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c116bb-eb9c-4881-ed05-2502a92162f5"
      },
      "source": [
        "stored.show(5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|           pos_begin|             pos_end|          pos_result|            pos_meta|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[0, 6, 9, 11, 16,...|[4, 7, 9, 14, 19,...|[NNP, VBZ, DT, RB...|[{word -> Peter},...|\n",
            "|My life in Russia...|[0, 3, 8, 11, 18,...|[1, 6, 9, 16, 19,...|[PRP$, NN, IN, NN...|[{word -> My}, {w...|\n",
            "|John and Peter ar...|[0, 5, 9, 15, 19,...|[3, 7, 13, 17, 26...|[NNP, CC, NNP, VB...|[{word -> John}, ...|\n",
            "|Lucas Nogal Dunbe...|[0, 6, 12, 23, 26...|[4, 10, 21, 24, 2...|[NNP, NNP, NNP, V...|[{word -> Lucas},...|\n",
            "|Europe is very cu...|[0, 7, 10, 15, 23...|[5, 8, 13, 21, 26...|[NNP, VBZ, RB, RB...|[{word -> Europe}...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI8rO1GjfDhz"
      },
      "source": [
        "---------\n",
        "## Spark SQL Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c5OVnNafDh0"
      },
      "source": [
        "from pyspark.sql.functions import *"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_nWknqlfDh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ec7de8-6c34-47f0-921c-8c9f3b2285a2"
      },
      "source": [
        "stored.filter(array_contains('pos_result', 'VBD')).show(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---------+-------+----------+--------+\n",
            "|text|pos_begin|pos_end|pos_result|pos_meta|\n",
            "+----+---------+-------+----------+--------+\n",
            "+----+---------+-------+----------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwBH_f-1fDh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69a0ba3-d2cb-47b9-ccf5-a33c4d101f75"
      },
      "source": [
        "stored.withColumn('token_count', size('pos_result')).select('pos_result', 'token_count').show(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----------+\n",
            "|          pos_result|token_count|\n",
            "+--------------------+-----------+\n",
            "|[NNP, VBZ, DT, RB...|          7|\n",
            "|[PRP$, NN, IN, NN...|          8|\n",
            "|[NNP, CC, NNP, VB...|         15|\n",
            "|[NNP, NNP, NNP, V...|         15|\n",
            "|[NNP, VBZ, RB, RB...|         15|\n",
            "+--------------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZn-kEFifDh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78630c36-90c9-40db-bf01-a61a2c1f7be1"
      },
      "source": [
        "stored.select('text', array_max('pos_end')).show(5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+------------------+\n",
            "|                text|array_max(pos_end)|\n",
            "+--------------------+------------------+\n",
            "|Peter is a very g...|                27|\n",
            "|My life in Russia...|                37|\n",
            "|John and Peter ar...|                76|\n",
            "|Lucas Nogal Dunbe...|                67|\n",
            "|Europe is very cu...|                68|\n",
            "+--------------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfzcYDcFfDiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2fdbb6a-cf8a-46a1-a738-28e8e25fbe48"
      },
      "source": [
        "stored.withColumn('unique_pos', array_distinct('pos_result')).select('pos_result', 'unique_pos').show(5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|          pos_result|          unique_pos|\n",
            "+--------------------+--------------------+\n",
            "|[NNP, VBZ, DT, RB...|[NNP, VBZ, DT, RB...|\n",
            "|[PRP$, NN, IN, NN...|[PRP$, NN, IN, NN...|\n",
            "|[NNP, CC, NNP, VB...|[NNP, CC, VBP, NN...|\n",
            "|[NNP, NNP, NNP, V...|[NNP, VBZ, DT, RB...|\n",
            "|[NNP, VBZ, RB, RB...|[NNP, VBZ, RB, JJ...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9k5hwUSfDiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bb3976-5f66-4c75-b901-c566ce3bd48f"
      },
      "source": [
        "stored.groupBy(array_distinct('pos_result')).count().show(10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------+-----+\n",
            "|array_distinct(pos_result)|count|\n",
            "+--------------------------+-----+\n",
            "|      [NNP, CC, VBP, NN...|    1|\n",
            "|      [NNP, VBZ, DT, RB...|    1|\n",
            "|      [NNP, VBZ, RB, JJ...|    1|\n",
            "|      [PRP$, NN, IN, NN...|    1|\n",
            "|      [NNP, VBZ, DT, RB...|    1|\n",
            "+--------------------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O_ERu47fDiI"
      },
      "source": [
        "----------------\n",
        "### SQL Functions with `col`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP2dz_BqfDiJ"
      },
      "source": [
        "from pyspark.sql.functions import col"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9a1KaVEfDiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3824ab0-240d-45af-fb1f-b77ed6733e45"
      },
      "source": [
        "stored.select(col('pos_meta').getItem(0).getItem('word')).show(5)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "|pos_meta[0][word]|\n",
            "+-----------------+\n",
            "|            Peter|\n",
            "|               My|\n",
            "|             John|\n",
            "|            Lucas|\n",
            "|           Europe|\n",
            "+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0TtaUgUfDiP"
      },
      "source": [
        "-------------\n",
        "### Spark NLP Annotation UDFs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psqxd7eWfDiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df97eb8-0127-4f56-83e7-75a646bce115"
      },
      "source": [
        "result.select('pos').show(1, truncate=False)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|pos                                                                                                                                                                                                                                                                    |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[{pos, 0, 4, NNP, {word -> Peter}, []}, {pos, 6, 7, VBZ, {word -> is}, []}, {pos, 9, 9, DT, {word -> a}, []}, {pos, 11, 14, RB, {word -> very}, []}, {pos, 16, 19, JJ, {word -> good}, []}, {pos, 21, 26, NN, {word -> person}, []}, {pos, 27, 27, ., {word -> .}, []}]|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miYDmJiSfDiU"
      },
      "source": [
        "def nn_tokens(annotations):\n",
        "    nn_annotations = list(\n",
        "        filter(lambda annotation: annotation.result == 'NN', annotations)\n",
        "    )\n",
        "    return list(\n",
        "        map(lambda nn_annotation: nn_annotation.metadata['word'], nn_annotations)\n",
        "    )"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsJDj_bjfDiX"
      },
      "source": [
        "from sparknlp.functions import *"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeYfUAbXfDid"
      },
      "source": [
        "from pyspark.sql.types import ArrayType, StringType"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMdSUHW-fDig",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "outputId": "a9234e3b-38ee-40a9-9e31-0b09e4934b5c"
      },
      "source": [
        "result.select(map_annotations(nn_tokens, ArrayType(StringType()))('pos').alias('nn_tokens')).show(truncate=False)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-eebf1be559ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArrayType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nn_tokens'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/sparknlp/functions.py\", line 9, in <lambda>\n    lambda content: f(content),\n  File \"<ipython-input-26-62e82ec04310>\", line 3, in nn_tokens\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 4356, in filter\n    return _invoke_higher_order_function(\"ArrayFilter\", [col], [f])\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 4183, in _invoke_higher_order_function\n    expressions = sc._jvm.org.apache.spark.sql.catalyst.expressions\nAttributeError: 'NoneType' object has no attribute '_jvm'\n"
          ]
        }
      ]
    }
  ]
}